import os
import pandas as pd
import logging
import requests
from datetime import datetime
from collections import defaultdict
from dotenv import load_dotenv

# === –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è ===
load_dotenv()

ACCESS_TOKEN = os.getenv("VK_TOKEN")
ACCOUNT_ID = os.getenv("VK_ACCOUNT_ID")

BASE_URL_V3 = "https://ads.vk.com/api/v3"
BASE_URL_V2 = "https://ads.vk.com/api/v2"
DOWNLOAD_DIR = "/opt/bot/master"

# === –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ===
logging.basicConfig(
    filename="/opt/bot/master/bot_master_solo.log",
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s"
)

# === –ë–∞–∑–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ä–∞—Å—á—ë—Ç–∞ –Ω–æ–º–µ—Ä–∞ –¥–Ω—è ===
BASE_DATE = datetime(2025, 7, 14)
BASE_NUMBER = 53

def get_day_number(today):
    delta = (today - BASE_DATE).days
    return BASE_NUMBER + delta

# === –û–ø—Ä–µ–¥–µ–ª—è–µ–º –≥—Ä—É–ø–ø—É –ø–æ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ ===
def get_output_filename(file_name, day_number):
    if "MFO5" in file_name:
        return f"–ë0 ({day_number}).txt", "–ë0"
    elif "6_web" in file_name:
        return None, "6_web"
    elif any(name in file_name for name in ["253", "345"]):
        return f"–ë1 ({day_number}).txt", "–ë1"
    elif "389" in file_name:
        return f"–ù1 ({day_number}).txt", "–ù1"
    elif "390" in file_name:
        return f"–ù2 ({day_number}).txt", "–ù2"
    else:
        return None, None

# === VK API ===
def upload_user_list(file_path, list_name):
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç TXT –∫–∞–∫ —Å–ø–∏—Å–æ–∫ —Ç–µ–ª–µ—Ñ–æ–Ω–æ–≤ –≤ VK ADS"""
    url = f"{BASE_URL_V3}/remarketing/users_lists.json"
    headers = {"Authorization": f"Bearer {ACCESS_TOKEN}"}
    files = {"file": open(file_path, "rb")}
    data = {"name": list_name, "type": "phones"}

    resp = requests.post(url, headers=headers, files=files, data=data)
    files["file"].close()
    result = resp.json()

    if resp.status_code != 200 or "error" in result:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Å–ø–∏—Å–∫–∞: {result}")
    return result.get("id")

def create_segment_with_list(segment_name, list_id):
    """–°–æ–∑–¥–∞—ë—Ç —Å–µ–≥–º–µ–Ω—Ç –≤ VK ADS –∏ –¥–æ–±–∞–≤–ª—è–µ—Ç —Ç—É–¥–∞ —Å–ø–∏—Å–æ–∫"""
    url = f"{BASE_URL_V2}/remarketing/segments.json"
    headers = {"Authorization": f"Bearer {ACCESS_TOKEN}", "Content-Type": "application/json"}

    payload = {
        "name": segment_name,
        "pass_condition": 1,
        "relations": [
            {"object_type": "remarketing_users_list", "params": {"source_id": list_id, "type": "positive"}}
        ]
    }

    resp = requests.post(url, headers=headers, json=payload)
    result = resp.json()
    if resp.status_code != 200 or "error" in result:
        raise Exception(f"–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–µ–≥–º–µ–Ω—Ç–∞: {result}")
    return result.get("id")

# === –û—Å–Ω–æ–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ CSV ===
def process_csv_files():
    logging.info("=== –ó–∞–ø—É—Å–∫ SOLO –æ–±—Ä–∞–±–æ—Ç–∫–∏ CSV ===")

    csv_files = [f for f in os.listdir(DOWNLOAD_DIR) if f.endswith(".csv")]
    if not csv_files:
        logging.info("–ù–µ—Ç CSV —Ñ–∞–π–ª–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏.")
        return

    today = datetime.today()
    day_number = get_day_number(today)
    logging.info(f"–ù–æ–º–µ—Ä –¥–Ω—è: {day_number}")

    output_data = defaultdict(list)
    group_stats = defaultdict(int)
    file_stats = []
    total_lines = 0

    for csv_file in csv_files:
        file_path = os.path.join(DOWNLOAD_DIR, csv_file)
        try:
            df = pd.read_csv(file_path)
            fname = os.path.basename(csv_file)
            output_name, group_key = get_output_filename(fname, day_number)

            # === –õ–æ–≥–∏–∫–∞ –¥–ª—è 6_web ===
            if group_key == "6_web":
                if 'phone' not in df.columns or 'channel_id' not in df.columns:
                    logging.error(f"–§–∞–π–ª {fname} –ø—Ä–æ–ø—É—â–µ–Ω ‚Äî –Ω–µ—Ç –Ω—É–∂–Ω—ã—Ö —Å—Ç–æ–ª–±—Ü–æ–≤.")
                    continue

                local_stats = defaultdict(int)

                for _, row in df.iterrows():
                    phone = str(row['phone']).replace("+", "").strip()
                    if not phone:
                        continue

                    ch = str(row['channel_id']).strip()
                    if ch == "15883":
                        group = "–ë–ë"
                    elif ch == "15686":
                        group = "–ë–ë –î–û–ü_1"
                    elif ch == "15273":
                        group = "–ë–ë –î–û–ü_2"
                    else:
                        group = "–ë–ë –î–û–ü_3"

                    output_data[group].append(phone)
                    group_stats[group] += 1
                    local_stats[group] += 1
                    total_lines += 1

                details = ", ".join([f"{g}: {c}" for g, c in local_stats.items()])
                file_stats.append(f"{fname}: {sum(local_stats.values())} —Å—Ç—Ä–æ–∫ ‚Üí {details}")

            # === –û–±—â–∞—è –ª–æ–≥–∏–∫–∞ ===
            else:
                if 'phone' not in df.columns:
                    logging.error(f"–§–∞–π–ª {fname} –ø—Ä–æ–ø—É—â–µ–Ω ‚Äî –Ω–µ—Ç –∫–æ–ª–æ–Ω–∫–∏ 'phone'.")
                    continue

                phones = df['phone'].dropna().astype(str)
                cleaned_phones = [p.replace("+", "").strip() for p in phones if p.strip()]
                lines_count = len(cleaned_phones)
                total_lines += lines_count

                if output_name:
                    output_data[group_key].extend(cleaned_phones)
                    group_stats[group_key] += lines_count
                    file_stats.append(f"{fname}: {lines_count} —Å—Ç—Ä–æ–∫ ‚Üí {group_key}")
                else:
                    file_stats.append(f"{fname}: –ø—Ä–æ–ø—É—â–µ–Ω (–Ω–µ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω)")

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ {csv_file}: {e}")

    # === –°–æ–∑–¥–∞—ë–º TXT —Ñ–∞–π–ª—ã ===
    txt_files = []
    for group_key, phones in output_data.items():
        filename = f"{group_key} ({day_number}).txt"
        txt_path = os.path.join(DOWNLOAD_DIR, filename)
        with open(txt_path, "w", encoding="utf-8") as f:
            f.write("\n".join(phones))
        txt_files.append(txt_path)
        logging.info(f"–°–æ–∑–¥–∞–Ω TXT: {filename}, {len(phones)} —Å—Ç—Ä–æ–∫")

    logging.info(f"=== –û–±—Ä–∞–±–æ—Ç–∫–∞ CSV –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –í—Å–µ–≥–æ —Å—Ç—Ä–æ–∫: {total_lines} ===")
    for stat in file_stats:
        logging.info(stat)

    # === –ó–∞–≥—Ä—É–∂–∞–µ–º TXT –≤ VK ADS ===
    for txt_path in txt_files:
        try:
            list_name = os.path.splitext(os.path.basename(txt_path))[0]
            list_id = upload_user_list(txt_path, list_name)
            segment_id = create_segment_with_list(f"–°–µ–≥–º–µ–Ω—Ç_{list_name}", list_id)
            logging.info(f"‚úÖ –ó–∞–ª–∏—Ç–æ –≤ VK ADS: {list_name} (ListID={list_id}, SegmentID={segment_id})")

            # –£–¥–∞–ª—è–µ–º —Ñ–∞–π–ª—ã
            os.remove(txt_path)
            logging.info(f"üßπ –£–¥–∞–ª—ë–Ω TXT: {txt_path}")

        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {txt_path}: {e}")

    # –£–¥–∞–ª—è–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ CSV
    for csv_file in csv_files:
        try:
            os.remove(os.path.join(DOWNLOAD_DIR, csv_file))
            logging.info(f"üßπ –£–¥–∞–ª—ë–Ω CSV: {csv_file}")
        except:
            pass

    logging.info("=== SOLO –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ ===")

if __name__ == "__main__":
    process_csv_files()
